{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bec8572-d3ef-481c-a1cd-f1aa3186e54e",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fd5ba0-679e-43dc-9700-d85202d965bf",
   "metadata": {},
   "source": [
    "Min-Max scaling, also known as Min-Max normalization, is a data preprocessing technique used to rescale the features of a dataset to a fixed range, usually [0, 1]. This is done to ensure that all features contribute equally to a model's performance and to prevent features with larger ranges from dominating those with smaller ranges.\n",
    "\n",
    "Formula-\n",
    "X_scaled = (X - X_min) / (X_max - X_min)\n",
    "\n",
    "Where:\n",
    "\n",
    "-X is the original value\n",
    "-X_min is the minimum value in the feature\n",
    "-X_max is the maximum value in the feature\n",
    "-X_scaled is the scaled value\n",
    "\n",
    "Example:\n",
    "\n",
    "### Example\n",
    "Suppose we have a dataset with a single feature representing the heights of people in centimeters: [150, 160, 170, 180, 190].\n",
    "\n",
    "1. Find the minimum and maximum values:  \n",
    "   - Minimum (\\(\\min(x)\\)): 150  \n",
    "   - Maximum (\\(\\max(x)\\)): 190  \n",
    "\n",
    "2. Apply the Min-Max scaling formula to each value:  \n",
    "   - For 150: \\( x' = \\frac{150 - 150}{190 - 150} = 0 \\)  \n",
    "   - For 160: \\( x' = \\frac{160 - 150}{190 - 150} = 0.25 \\)  \n",
    "   - For 170: \\( x' = \\frac{170 - 150}{190 - 150} = 0.5 \\)  \n",
    "   - For 180: \\( x' = \\frac{180 - 150}{190 - 150} = 0.75 \\)  \n",
    "   - For 190: \\( x' = \\frac{190 - 150}{190 - 150} = 1 \\)  \n",
    "\n",
    "The transformed dataset is [0, 0.25, 0.5, 0.75, 1]. \n",
    "\n",
    "Min-Max scaling is particularly useful in algorithms that rely on distance calculations, like k-nearest neighbors (KNN) or clustering algorithms, as it ensures all features are treated equally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f9f85a-d59c-4dc8-9ede-35ac021aacce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5bf209b-56bd-4c74-82c1-78d913e1ee2e",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1194b07-21d0-4c19-bb8f-9af09e835a83",
   "metadata": {},
   "source": [
    "The Unit Vector technique, also known as normalization or vector normalization, is a feature scaling method that transforms the features of a dataset so that each data point is represented as a unit vector. This means each feature vector is scaled to have a magnitude (or Euclidean norm) of 1. This is particularly useful when the direction of the data is more important than its magnitude.\n",
    "\n",
    "Formula:\n",
    "The Unit Vector scaling for a feature vector x is given by:\n",
    "x' = x / ||x||\n",
    "\n",
    "where:\n",
    "\n",
    "- x is the original feature vector.\n",
    "- ||x|| is the Euclidean norm (magnitude) of the vector x.\n",
    "- x' is the normalized vector.\n",
    "\n",
    "Difference from Min-Max Scaling:\n",
    "\n",
    "1)Objective: Unit Vector scaling normalizes data based on its magnitude, making all data points have a unit length, while Min-Max scaling rescales data to a specific range, such as [0, 1].\n",
    "\n",
    "2)Use Case: Unit Vector is useful when the direction of data is important (e.g., cosine similarity), whereas Min-Max scaling is useful for ensuring all features contribute equally to distance-based algorithms.\n",
    "\n",
    "\n",
    "Example:\n",
    "Consider a dataset with two features: the vector x = [3, 4].\n",
    "\n",
    "1)Calculate the Euclidean norm of x:\n",
    "||x|| = sqrt(3^2 + 4^2) = sqrt(9 + 16) = sqrt(25) = 5\n",
    "\n",
    "2)Normalize the vector x using Unit Vector scaling:\n",
    "x' = [3, 4] / 5 = [0.6, 0.8]\n",
    "\n",
    "The transformed vector is [0.6, 0.8], which has a magnitude of 1.\n",
    "\n",
    "Unit Vector scaling is particularly useful in applications where the angle or direction of vectors is more important than their magnitude, such as text classification using cosine similarity.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b39558-6942-4d47-bfe5-416548e091bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3209fef0-857f-49f3-b38a-a1d88caa10ce",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62b3fcd-e04c-4ff9-ab19-6a7e5538f32f",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a statistical technique used for dimensionality reduction. It transforms a dataset with many features into a smaller set of uncorrelated features called principal components, while retaining as much of the original data's variance as possible.\n",
    "\n",
    "How PCA Works:\n",
    "\n",
    "1)Standardization: Center the data by subtracting the mean and scaling to unit variance if necessary. This ensures that the PCA is not biased by the scale of the features.\n",
    "\n",
    "2)Covariance Matrix Computation: Compute the covariance matrix of the data to understand how the features vary with respect to each other.\n",
    "\n",
    "3)Eigenvectors and Eigenvalues Calculation: Calculate the eigenvectors and eigenvalues of the covariance matrix to identify the principal components. Eigenvectors represent directions in the feature space, and eigenvalues indicate the amount of variance in the data along these directions.\n",
    "\n",
    "4)Sort and Select Principal Components: Sort the eigenvectors by their corresponding eigenvalues in descending order. Select the top k eigenvectors, where k is the number of dimensions you want to retain.\n",
    "\n",
    "5)Transform the Data: Project the original data onto the new feature space using the selected eigenvectors to obtain the reduced dataset.\n",
    "\n",
    "Application in Dimensionality Reduction:\n",
    "\n",
    "PCA is used to reduce the dimensionality of a dataset while preserving as much variance as possible. This is helpful in reducing computational complexity, eliminating noise, and making data visualization easier.\n",
    "\n",
    "Example:\n",
    "\n",
    "Consider a dataset with three features: x1, x2, and x3. Hereâ€™s a simple illustration of how PCA might work:\n",
    "1)Data Matrix: Assume we have a data matrix X with 3 features.\n",
    "X = [[2.5, 2.4, 0.5],\n",
    "     [0.5, 0.7, 1.2],\n",
    "     [2.2, 2.9, 1.1],\n",
    "     [1.9, 2.2, 0.3],\n",
    "     [3.1, 3.0, 1.4],\n",
    "     [2.3, 2.7, 0.7],\n",
    "     [2.0, 1.6, 0.9],\n",
    "     [1.0, 1.1, 0.4],\n",
    "     [1.5, 1.6, 0.6],\n",
    "     [1.1, 0.9, 0.2]]\n",
    "\n",
    "2)Standardize the Data: Center and scale the data.\n",
    "\n",
    "3)Covariance Matrix: Compute the covariance matrix of X.\n",
    "\n",
    "4)Eigenvectors and Eigenvalues: Calculate the eigenvectors and eigenvalues of the covariance matrix.\n",
    "\n",
    "5)Select Principal Components: Choose the top k eigenvectors based on the largest eigenvalues. For example, if we select k = 2, we choose the two eigenvectors corresponding to the largest eigenvalues.\n",
    "\n",
    "6)Transform the Data: Project the original data onto the new 2-dimensional space formed by these two principal components.\n",
    "X_reduced = X * W\n",
    "where W is the matrix of selected eigenvectors.\n",
    "\n",
    "After applying PCA, the data is transformed from a 3-dimensional space to a 2-dimensional space, capturing most of the variance. This reduced dataset can be used for further analysis, visualization, or as input to machine learning algorithms.\n",
    "\n",
    "PCA is widely used in fields like image compression, genomics, and finance to reduce the complexity of data while retaining essential patterns.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1df9c8-ec6a-4378-b702-fd82a39f36fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ca44e53-8b35-4cd2-b944-45906cdb7c5b",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d3e609-4891-4d4b-8bcc-52899ffb585f",
   "metadata": {},
   "source": [
    " PCA (Principal Component Analysis) is a technique that can be used for feature extraction, and it plays a significant role in dimensionality reduction. Here's the relationship between PCA and feature extraction, along with an example to illustrate the concept:\n",
    "\n",
    "Relationship between PCA and Feature Extraction:\n",
    "\n",
    "1)Dimensionality Reduction: Both PCA and feature extraction are methods to reduce the number of features (dimensions) in a dataset. This reduction is essential when dealing with high-dimensional data, as it can simplify analysis, visualization, and modeling.\n",
    "\n",
    "2)Information Compression: PCA and feature extraction aim to preserve the most important information in the data while discarding less important or redundant features. They achieve this by creating new features that are combinations of the original features.\n",
    "\n",
    "3)Orthogonal Transformation: PCA transforms the original features into a new set of orthogonal (uncorrelated) features called principal components. This orthogonal transformation simplifies the representation of data.\n",
    "\n",
    "Using PCA for Feature Extraction (Example):\n",
    "\n",
    "Let's consider an example using image data. Suppose you have a dataset of grayscale images of handwritten digits, each represented as a 28x28 pixel grid, resulting in 784 features (one for each pixel). These high-dimensional features can be challenging to work with. You want to extract meaningful features to represent the images more compactly.\n",
    "\n",
    "Here's how PCA can be used for feature extraction in this context:\n",
    "\n",
    "1)Data Preparation: You start with a dataset of handwritten digit images, each represented as a 28x28 matrix, resulting in 784 pixel values for each image. You normalize these pixel values to have zero mean (subtract the mean) to ensure that PCA is not biased by differences in brightness.\n",
    "\n",
    "2)Applying PCA: You apply PCA to the dataset. PCA calculates the principal components, which are linear combinations of the original pixel values. These principal components are ranked in order of their ability to explain the variance in the data.\n",
    "\n",
    "3)Variance Explained: You can decide how many principal components to retain based on the percentage of variance you want to explain. For example, if you want to retain 95% of the variance, you select the top principal components that collectively explain at least 95% of the total variance in the data.\n",
    "\n",
    "4)Reduced-Dimension Representation: The selected principal components form a new set of features that are used to represent the images in a reduced-dimensional space. These features are typically much fewer than the original 784 pixels.\n",
    "\n",
    "5)Visualization or Analysis: You can use these extracted features for various purposes, such as visualization, clustering, or classification. The reduced-dimensional representation simplifies the analysis and often improves the efficiency and performance of machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e65353c-cf98-44ec-820b-cc1723ae47e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ff78691-1c35-484d-af61-3ee96b2a252d",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f999ddb-4b03-4f9a-9df1-8ecc1505099e",
   "metadata": {},
   "source": [
    " Using Min-Max scaling to preprocess the data in a recommendation system for a food delivery service can be beneficial because it helps ensure that all the features are on a similar scale, making it easier for the recommendation algorithm to learn from the data. Here's how you would use Min-Max scaling to preprocess the dataset:\n",
    "\n",
    "1. Identify the Features:\n",
    "In your dataset, you mentioned that you have features such as price, rating, and delivery time. These are the features that you want to scale using Min-Max scaling.\n",
    "\n",
    "2. Determine the Range:\n",
    "Decide on the range to which you want to scale the features. The typical range for Min-Max scaling is [0, 1], but you can choose a different range if it's more suitable for your specific use case.\n",
    "\n",
    "3. Calculate the Minimum and Maximum Values:\n",
    "For each of the features (price, rating, and delivery time), calculate the minimum and maximum values within the dataset. This involves finding the minimum and maximum values for each feature across all the data points.\n",
    "\n",
    "4. Apply Min-Max Scaling:\n",
    "Use the Min-Max scaling formula for each feature individually:\n",
    "For feature X:\n",
    "\n",
    "Xscaled = (X - Xmin) / (Xmax - Xmin)\n",
    "\n",
    "X is the original value of the feature.\n",
    "Xscaled is the scaled value of the feature.\n",
    "Xmin is the minimum value of the feature in the dataset.\n",
    "Xmax is the maximum value of the feature in the dataset.\n",
    "Apply this formula to each data point for each feature. After scaling, each feature will have values between 0 and 1 (or within your specified range).\n",
    "\n",
    "5. Updated Dataset:\n",
    "\n",
    "Replace the original values of price, rating, and delivery time with their scaled counterparts in the dataset. The dataset is now ready for use in building the recommendation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dccf4c-1a93-458d-89f4-5a24ef7e69df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "466a33f4-2051-41ae-b6b3-abcbd639f625",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8b359b-5f2c-4942-aea5-df3b8134a45d",
   "metadata": {},
   "source": [
    " Using PCA (Principal Component Analysis) to reduce the dimensionality of a dataset in a stock price prediction project can be advantageous, especially when dealing with a large number of features. Dimensionality reduction using PCA can help simplify the dataset, remove noise, and potentially improve the performance of your stock price prediction model. Here's a step-by-step guide on how to use PCA for this purpose:\n",
    "\n",
    "1)Data Preprocessing:\n",
    "Begin by collecting and preprocessing your dataset. This may involve gathering financial data for various companies and market trends. Ensure that your data is clean, missing values are handled, and all features are properly scaled.\n",
    "\n",
    "2)Standardization:\n",
    "Standardize your dataset by subtracting the mean and scaling to unit variance. PCA is sensitive to the scale of the features, so standardization is important to ensure that all features have similar influence during dimensionality reduction.\n",
    "\n",
    "3)Covariance Matrix Calculation:\n",
    "Compute the covariance matrix of your standardized data. The covariance matrix describes the relationships and dependencies between pairs of features in your dataset.\n",
    "\n",
    "4)Eigendecomposition:\n",
    "Perform eigendecomposition (eigenvalue decomposition) on the covariance matrix to obtain the eigenvalues and eigenvectors. The eigenvectors represent the principal components, and the eigenvalues indicate the amount of variance explained by each principal component.\n",
    "\n",
    "5)Principal Component Selection:\n",
    "Sort the eigenvalues in descending order and select a subset of the top (k) eigenvectors based on how much variance you want to retain in the reduced dataset. You can choose a threshold for explained variance (e.g., 95% of the total variance) to determine the number of principal components to keep.\n",
    "\n",
    "6)Projection:\n",
    "Project your original data onto the selected principal components to obtain the reduced-dimensional dataset. This is done by taking the dot product of your standardized data with the selected eigenvectors.\n",
    "\n",
    "7)Model Building:\n",
    "Use the reduced-dimensional dataset as input to train your stock price prediction model. This lower-dimensional representation often simplifies model training and may reduce the risk of overfitting.\n",
    "\n",
    "8)Model Evaluation:\n",
    "Evaluate the performance of your model using appropriate metrics and techniques. Since you've reduced the dimensionality of the data, it's important to assess how well the reduced features capture the essential information for stock price prediction.\n",
    "\n",
    "9)Interpretability:\n",
    "Analyze the principal components to understand which original features contribute the most to each principal component. This can provide insights into the most influential factors affecting stock prices.\n",
    "\n",
    "10)Fine-Tuning:\n",
    "Depending on the model's performance and your goals, you can experiment with different numbers of principal components and refine your model accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc616e4b-526c-4f76-a7b3-960f483e60e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d242e2a5-d5a6-4b7a-97e6-79e35a74f196",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e005f9a-3ea0-493b-98ff-be7a837d8b93",
   "metadata": {},
   "source": [
    "Min-Max Scaling to Range -1 to 1-\n",
    "\n",
    "Understanding the Formula-\n",
    "\n",
    "The standard Min-Max scaling formula is:\n",
    "X_scaled = (X - X_min) / (X_max - X_min)\n",
    "\n",
    "This scales values to the range of 0 to 1. To scale to -1 to 1, we modify it as follows:\n",
    "X_scaled = 2 * ((X - X_min) / (X_max - X_min)) - 1\n",
    "\n",
    "Applying the Formula\n",
    "Given the dataset: [1, 5, 10, 15, 20]\n",
    "\n",
    "-X_min = 1\n",
    "-X_max = 20\n",
    "\n",
    "Let's apply the formula to each value:\n",
    "\n",
    "For 1:\n",
    "X_scaled = 2 * ((1 - 1) / (20 - 1)) - 1 = -1\n",
    "\n",
    "For 5:\n",
    "X_scaled = 2 * ((5 - 1) / (20 - 1)) - 1 = -0.63\n",
    "\n",
    "For 10:\n",
    "X_scaled = 2 * ((10 - 1) / (20 - 1)) - 1 = -0.37\n",
    "\n",
    "For 15:\n",
    "X_scaled = 2 * ((15 - 1) / (20 - 1)) - 1 = 0.37\n",
    "\n",
    "For 20:\n",
    "X_scaled = 2 * ((20 - 1) / (20 - 1)) - 1 = 1\n",
    "\n",
    "Scaled Dataset\n",
    "The scaled dataset is: [-1, -0.63, -0.37, 0.37, 1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0fb169-63bc-4821-b3ff-e2db1a0c78f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24a65205-37ae-4325-8efc-1e08f0c62180",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc112c8-4393-4fe8-8a4f-2b33aae2ffbc",
   "metadata": {},
   "source": [
    "To perform feature extraction using Principal Component Analysis (PCA) on a dataset containing features like [height, weight, age, gender, blood pressure], follow these steps to determine how many principal components to retain:\n",
    "\n",
    "1.Preprocess the Data: \n",
    "   - Convert categorical variables (like gender) into numerical format if needed (e.g., one-hot encoding).\n",
    "   - Standardize the features to have zero mean and unit variance.\n",
    "\n",
    "2. Compute Covariance Matrix: \n",
    "   - Calculate the covariance matrix of the standardized features to understand the relationships between them.\n",
    "\n",
    "3. Calculate Eigenvectors and Eigenvalues: \n",
    "   - Compute the eigenvectors and eigenvalues of the covariance matrix. Eigenvectors represent the principal components, and eigenvalues indicate the variance captured by each component.\n",
    "\n",
    "4. Select Principal Components: \n",
    "   - Calculate the cumulative explained variance ratio for each principal component.\n",
    "   - Determine how many components to retain by choosing the smallest number of components that capture a sufficiently large percentage of the total variance. This is typically chosen to be 95% or 99%.\n",
    "\n",
    "Choosing the Number of Principal Components:\n",
    "\n",
    "- Calculate Explained Variance Ratio: \n",
    "  - Compute the explained variance ratio for each principal component and then calculate the cumulative explained variance ratio.\n",
    "\n",
    "- Determine Retention: \n",
    "  - Select the number of principal components that collectively account for at least 95% (or 99%) of the total variance.\n",
    "\n",
    "Example:\n",
    "\n",
    "1. Compute Explained Variance Ratios:\n",
    "   - Suppose the explained variance ratios for the principal components are as follows: [0.45, 0.30, 0.15, 0.07, 0.03].\n",
    "\n",
    "2. Calculate Cumulative Explained Variance:\n",
    "   - Cumulative explained variance ratios: [0.45, 0.75, 0.90, 0.97, 1.00].\n",
    "\n",
    "3. Select Components:\n",
    "   - To retain at least 95% of the variance, you would choose the first 3 principal components, as they account for 90% of the variance, and the fourth component brings it up to 97%.\n",
    "\n",
    "In this case, you would choose to retain **3 or 4 principal components**. The exact number depends on how much variance you aim to retain. For most applications, retaining components that explain 95% to 99% of the variance is considered sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eccf2f-47b0-4a47-abb4-553da3be27f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7b2bc7-db7b-40c9-9a4a-0e3d861a672b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fba015f-ca0c-4c5c-ad8b-37992cc2f0bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967a0e7b-0ffd-4979-93a1-cdb88ff8526b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317a0f5a-ea7d-4583-a614-be2bc075b0b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e919f73-69cd-45b7-bab6-3b879e18cf4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7a7dd5-3015-4321-8926-9df623b22335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828ccb01-1ca4-4ae5-9f3a-1be7c6c77f59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be3a968-9a1c-4517-967b-27a0ed835e25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b08ea3-1f00-48b9-880e-8446ceb27619",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9a9647-86b4-4717-85d3-d9de615c53f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f5af19-a075-43f7-9b43-5ca9c70ecffb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c94c04-465c-4a09-a385-6f7ff0fd5472",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2ccd0a-b767-43ba-aebb-a6ffcb68bcd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60afe79-10d0-43c2-bd81-c2ed4f26199c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e68d2b-b10d-46ec-b451-a249a3a6feaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50b014b-7efd-4c00-bc58-b20b55ccf086",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaa0797-c103-4310-ace5-0f9cb8524f5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b79c1da-90f8-4f93-862a-e6cc1f7f416b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c86e10-6e23-4b2a-b650-1171247b52a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b868e3e8-d6be-4749-ad32-451a5333bb15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9490ce81-d6d7-4c13-9f71-542dc9dbf3a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0217abe8-20bb-497d-8dbb-3858a9a0272d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e0b09c-cac0-4c76-b539-520d8662e3cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341e8a34-c26d-458a-ad3c-4ad80b597fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5504e8-4a9b-490c-999d-ac399c4fca9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
